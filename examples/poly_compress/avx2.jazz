param int KYBER_N = 256;
param int KYBER_Q = 3329;
u16[16] jvx16 = {20159, 20159, 20159, 20159, 20159, 20159, 20159, 20159,
                 20159, 20159, 20159, 20159, 20159, 20159, 20159, 20159};
u16 pc_shift1_s = 0x200;
u16 pc_mask_s = 0x0F;
u16 pc_shift2_s = 0x1001;
u32[8] pc_permidx_s = {0,4,1,5,2,6,3,7};

fn _poly_csubq(reg ptr u16[KYBER_N] rp) -> reg ptr u16[KYBER_N]
{
  reg u16 t;
  reg u16 b;
  inline int i;

  for i=0 to KYBER_N
  {
    t = rp[i];
    t -= KYBER_Q;
    b = t;
    b >>s= 15;
    b &= KYBER_Q;
    t += b;
    rp[i] = t;
  }
  return rp;
}

fn _poly_compress_avx2(reg u64 rp, reg ptr u16[KYBER_N] a) -> reg ptr u16[KYBER_N]
{
  inline int i;
  reg u256 f0 f1 f2 f3 v shift1 mask shift2 permidx;
  reg ptr u16[16] x16p;

  a = _poly_csubq(a);

  x16p = jvx16;
  v = x16p[u256 0];
  shift1 = #VPBROADCAST_16u16(pc_shift1_s);
  mask = #VPBROADCAST_16u16(pc_mask_s);
  shift2 = #VPBROADCAST_16u16(pc_shift2_s);
  permidx = pc_permidx_s[u256 0];

  for i=0 to KYBER_N/64
  {
    f0 = a[u256 4*i];
    f1 = a[u256 4*i + 1];
    f2 = a[u256 4*i + 2];
    f3 = a[u256 4*i + 3];
    f0 = #VPMULH_16u16(f0, v);
    f1 = #VPMULH_16u16(f1, v);
    f2 = #VPMULH_16u16(f2, v);
    f3 = #VPMULH_16u16(f3, v);
    f0 = #VPMULHRS_16u16(f0, shift1);
    f1 = #VPMULHRS_16u16(f1, shift1);
    f2 = #VPMULHRS_16u16(f2, shift1);
    f3 = #VPMULHRS_16u16(f3, shift1);
    f0 = #VPAND_256(f0, mask);
    f1 = #VPAND_256(f1, mask);
    f2 = #VPAND_256(f2, mask);
    f3 = #VPAND_256(f3, mask);
    f0 = #VPACKUS_16u16(f0, f1);
    f2 = #VPACKUS_16u16(f2, f3);
    f0 = #VPMADDUBSW_256(f0, shift2);
    f2 = #VPMADDUBSW_256(f2, shift2);
    f0 = #VPACKUS_16u16(f0, f2);
    f0 = #VPERMD(permidx, f0);
    (u256)[rp + 32*i] = f0;
  }

  return a;
}

export fn poly_compress_avx2(reg u64 rp, reg u64 a) {
  reg u16 t;
  stack u16[KYBER_N] p;
  reg ptr u16[KYBER_N] pp;
  inline int i;

  for i=0 to KYBER_N {
      t = (u16)[a + 2*i];
      p[i] = t;
  }

  pp = p;
  pp = _poly_compress_avx2(rp, pp);
}
